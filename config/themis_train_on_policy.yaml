defaults:
    - _self_
    - environment
    - agent: ppo
    
# Experiment level parameters
experiment_type: State-Vis-Pretraining
test: IMDP_Offline_NO_Freeze
device: cpu
offline_epochs: 100
num_seed_steps: 0 # Steps that the agent does random actions to fill the buffers 5e3 for continuous
num_unsup_steps: 0 #5000
num_train_steps: 5e5 #1e6 for continuous
# replay_buffer_capacity: 1e4 #1e6
deploy_mode: True

import_model: True
import_protocol: NORMAL
export_protocol: NORMAL
freeze_protocol: NO

# unsup training
gradient_update: 1
topK: 5

# reward learning
learn_reward: True
segment: 50
activation: tanh
num_interact: 4000 # After how many steps we retrain the reward model
reward_model_capacity: 10000 # How many feedbacks can it hold
reward_lr: 0.003
reward_batch: 50 # How many segments will be generated for human input
reward_update: 200 # How many epochs the reward model is training for
feed_type: 0 # the sampling method used
ensemble_size: 3
max_feedback: 1400 # Max total number of interactions (incremented by reward_batch)
large_batch: 10
label_margin: 0.0
reward_scale: 1.0
reward_intercept: 0.0
human_teacher: False # Use human input on True, uses synthetic feedback on False
teacher_beta: -1    # Rationality Constant 0 leads to random choices and infinite (-1) to perfectly rational
teacher_gamma: 1    # Discound Factor for myopic behavior paying more attention on recent states
teacher_eps_mistake: 0  # Probability of mistake
teacher_eps_skip: 0     # Probability of considering both segments bad
teacher_eps_equal: 0    # Probability of considering both segments equally good

# scheduling
reward_schedule: 0

# evaluation config
eval_frequency: 10000 #100
num_eval_episodes: 10

# logger
log_frequency: 1000
log_save_tb: true

# video recorder (Use only in evaluation)
save_video: false
video_location: ${models_dir}/trained_env_eval_clips/
record_frequency: 1

# setups
seed: 1
debug: False
log_success: True

#Xplain Params
checkpoint_frec: 500 # No. of steps to 
checkpoints_dir: ${models_dir}/checkpoints/
xplain_action: False
xplain_state: False

models_dir: ${output_dir}/${agent.name}_${test}/seed-${seed}

offline_models_dir: ${models_dir}/

# All output from experiments
output_dir: results/${experiment_type}/${domain}/${env}
